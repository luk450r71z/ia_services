from typing import Dict, Any, List
from langchain_core.messages import HumanMessage, AIMessage
from langchain_groq import ChatGroq
import os
import json
import logging

from ..models.conversation_models import ConversationState
from ..utils.env_utils import load_env_variables

logger = logging.getLogger(__name__)

# Cargar variables de entorno al importar el mÃ³dulo
load_env_variables()


class QuestionnaireAgent:
    """
    Este agente maneja questionarios automatizados de manera secuencial,
    recopila respuestas, decide cuÃ¡ndo repreguntar y envÃ­a resÃºmenes por correo.
    """
    
    def __init__(self, content: Dict[str, Any] = None):
        """
        Inicializa el agente de Questionnaire.
        
        Args:
            content: Contenido completo de la sesiÃ³n (welcome_message, questions, etc.)
        """
        self.state = ConversationState()
        self.initialized = False
        self.content = content or {}  # Inicializar content como diccionario vacÃ­o si es None
        
        # Debug: Log del content recibido
        logger.info(f"ğŸ”§ DEBUG: Inicializando agente con questions_data={len(content.get('questions', []) if content else [])} preguntas")
        
        # Extraer y procesar questions del content
        questions_data = content.get('questions', []) if content else []
        try:
            if questions_data:
                self.questions_data = self.extract_questions(questions_data)
                self.questions = [q["question"] for q in self.questions_data]
                logger.info(f"âœ… Preguntas extraÃ­das exitosamente: {len(self.questions)} preguntas")
            else:
                self.questions_data = []
                self.questions = []
                logger.warning("âš ï¸ No se proporcionaron preguntas en el content")
        except Exception as e:
            logger.error(f"âŒ Error extrayendo preguntas: {str(e)}")
            self.questions_data = []
            self.questions = []
        
        # Guardar preguntas en metadatos
        if self.questions:
            self.state.extra_data["questions_data"] = self.questions_data
    
    @staticmethod
    def extract_questions(questions_data: Any) -> List[Dict[str, Any]]:
        """
        Extrae preguntas inteligentemente de cualquier formato JSON usando LLM.
        
        Args:
            questions_data: Datos en cualquier formato (array, object, nested)
            
        Returns:
            Lista de diccionarios con pregunta y opciones (si las hay)
            Formato: [{"question": "Â¿Pregunta?", "options": ["a", "b"] o None}]
            
        Raises:
            ValueError: Si no se puede extraer preguntas o no hay LLM disponible
        """
        # Convertir a JSON string
        json_str = json.dumps(questions_data, indent=2, ensure_ascii=False)
        
        # Configurar LLM (obligatorio)
        load_env_variables()
        groq_api_key = os.getenv("GROQ_API_KEY")
        
        if not groq_api_key:
            raise ValueError("GROQ_API_KEY es requerida para extraer preguntas inteligentemente")
        
        llm = ChatGroq(api_key=groq_api_key, model="llama3-8b-8192")
        
        prompt = f"""JSON_INPUT:
{json_str}

TASK: Extract questions and return JSON array only.

OUTPUT_FORMAT:
[{{"question": "text", "options": null}}, {{"question": "text", "options": ["a","b"]}}]

RETURN ONLY JSON ARRAY - NO OTHER TEXT"""
        
        response = llm.invoke(prompt)
        content = response.content.strip() if hasattr(response, 'content') else str(response).strip()
        
        try:
            # Extraer JSON de la respuesta (puede haber texto adicional)
            json_start = content.find('[')
            json_end = content.rfind(']') + 1
            
            if json_start == -1 or json_end == 0:
                raise ValueError("No se encontrÃ³ array JSON en la respuesta")
            
            json_content = content[json_start:json_end]
            questions_with_options = json.loads(json_content)
            
            if not questions_with_options:
                raise ValueError("No se encontraron preguntas en el JSON proporcionado")
            
            return questions_with_options
            
        except json.JSONDecodeError as e:
            raise ValueError(f"Error parseando JSON: {json_content[:100]}...")
        except Exception as e:
            raise ValueError(f"Error procesando respuesta del LLM: {str(e)}")
    
    def start_conversation(self, session_data: Dict[str, Any] = None) -> str:
        """
        Inicia una nueva conversaciÃ³n.
        
        Args:
            session_data: Datos de la sesiÃ³n (opcional)
        
        Returns:
            Mensaje inicial del agente
        """
        # Debug: Log del estado del agente
        logger.info(f"ğŸ”§ DEBUG: start_conversation - questions_count={len(self.questions)}")
        
        # Verificar que hay preguntas configuradas
        if not self.questions:
            logger.error("âŒ No hay preguntas configuradas en el agente")
            return "Error: No questions have been configured for this questionnaire. Please contact the administrator."
        
        # Configurar estado inicial
        self.state.pending_questions = self.questions
        self.state.current_question_index = 0
        self.state.current_question = self.questions[0]
        
        # Obtener mensaje de bienvenida del content y reemplazar variables
        welcome_content = self.content.get('welcome_message', 'Hello, I am Adaptiera\'s Questionnaire Assistant. I will ask you some questions to get to know you better. Please answer as honestly as possible. Let\'s begin:')
        
        # Reemplazar {client_name} con el nombre real del cliente
        client_name = self.content.get('client_name', '')
        welcome_content = welcome_content.replace('{client_name}', client_name)
        
        welcome_message = AIMessage(content=welcome_content)
        self.state.messages.append(welcome_message)
        
        # Primera pregunta
        formatted_question = self.questions[0]
        question_message = AIMessage(content=formatted_question)
        self.state.messages.append(question_message)
        
        self.initialized = True
        return f"{welcome_message.content}\n\n{formatted_question}"
    
    def process_user_input(self, user_input: str) -> str:
        """
        Procesa la entrada del usuario y retorna la respuesta del agente.
        
        Args:
            user_input: Mensaje del usuario
            
        Returns:
            Respuesta del agente
        """
        if not self.initialized:
            return self.start_conversation()
        
        # Verificar si la conversaciÃ³n ya estÃ¡ completa
        if self.state.conversation_complete:
            return "The questionnaire has already ended. Thank you for your participation!"
        
        # Agregar mensaje del usuario al estado
        user_message = HumanMessage(content=user_input)
        self.state.messages.append(user_message)
        
        # Evaluar la respuesta
        is_satisfactory, clarification_reason = self._evaluate_response(user_input)
        
        if is_satisfactory:
            # Guardar respuesta satisfactoria en el estado de la conversaciÃ³n
            self.state.user_responses[self.state.current_question] = user_input
            
            # Actualizar estado de la conversaciÃ³n
            self.state.needs_clarification = False
            self.state.clarification_reason = None
            
            # Avanzar a la siguiente pregunta
            return self._next_question()
        else:
            # Solicitar aclaraciÃ³n
            self.state.needs_clarification = True
            self.state.clarification_reason = clarification_reason
            
            clarification_message = AIMessage(content=f"""I would like you to expand on your previous response.
{clarification_reason}

Please provide more details about: {self.state.current_question}""")
            
            self.state.messages.append(clarification_message)
            return clarification_message.content
    
    def _evaluate_response(self, user_response: str) -> tuple[bool, str]:
        """
        EvalÃºa si la respuesta del usuario es satisfactoria.
        """
        current_question = self.state.current_question
        
        # Para todas las preguntas, usar evaluaciÃ³n con LLM
        return self._evaluate_open_question(user_response, current_question)
    
    def _evaluate_open_question(self, user_response: str, current_question: str) -> tuple[bool, str]:
        """Evaluates open-ended question with LLM"""
        load_env_variables()
        groq_api_key = os.getenv("GROQ_API_KEY")
        
        if not groq_api_key:
            # Simple fallback for open questions
            is_satisfactory = len(user_response.strip()) > 3
            clarification_reason = "Please provide a more detailed response." if not is_satisfactory else ""
            return is_satisfactory, clarification_reason
        
        llm = ChatGroq(api_key=groq_api_key, model="llama3-8b-8192")
        
        prompt = f"""
        Evaluate if the following response is satisfactory for the given question:
        
        Question: {current_question}
        Response: {user_response}
        
        Respond ONLY with:
        - "SATISFACTORY" if the response provides basic relevant information
        - "NEEDS_CLARIFICATION: [specific reason]" if it's empty, irrelevant, or very confusing
        
        Be LENIENT in your evaluation. Accept responses that have at least some relation to the question, 
            even if they are brief or not very detailed. Only request clarification if the response really 
            doesn't make sense or is completely off-topic.
        """
        
        try:
            response = llm.invoke(prompt)
            content = response.content.strip() if hasattr(response, 'content') else str(response).strip()
            
            if content.startswith("SATISFACTORY"):
                return True, ""
            elif content.startswith("NEEDS_CLARIFICATION"):
                reason = content.replace("NEEDS_CLARIFICATION:", "").strip()
                return False, reason
            else:
                # Formato inesperado, ser permisivo
                return True, ""
        except Exception as e:
            # Fallback simple
            is_satisfactory = len(user_response.strip()) > 3
            clarification_reason = "Please provide a more detailed response." if not is_satisfactory else ""
            return is_satisfactory, clarification_reason
    
    def _next_question(self) -> str:
        """
        Avanza a la siguiente pregunta o finaliza la conversaciÃ³n.
        
        Returns:
            Mensaje con la siguiente pregunta o finalizaciÃ³n
        """
        self.state.current_question_index += 1
        
        if self.state.current_question_index < len(self.state.pending_questions):
            # Hay mÃ¡s preguntas
            self.state.current_question = self.state.pending_questions[self.state.current_question_index]
            
            # Obtener siguiente pregunta
            formatted_question = self.questions[self.state.current_question_index]
            
            next_question_message = AIMessage(content=f"""Perfect, thank you for your response.

Next question:
{formatted_question}""")
            
            self.state.messages.append(next_question_message)
            return next_question_message.content
        else:
            # No hay mÃ¡s preguntas, finalizar conversaciÃ³n
            return self._finalize_conversation()
    
    def _finalize_conversation(self) -> str:
        """
        Finaliza la conversaciÃ³n guardando respuestas y enviando correo.
        
        Returns:
            Mensaje de finalizaciÃ³n
        """
        self.state.conversation_complete = True
        self.state.current_question = None
        
        # Las notificaciones ahora se manejan automÃ¡ticamente por el websocket_manager
        # cuando la conversaciÃ³n se marca como completa - no necesitamos envÃ­o manual aquÃ­
        
        final_message = AIMessage(content="""Thank you very much for your time! 

âœ… Your responses have been recorded.

âœ… Our team will review your information and contact you soon.

Have a great day!""")
        
        self.state.messages.append(final_message)
        return final_message.content
    
    def is_conversation_complete(self) -> bool:
        """
        Verifica si la conversaciÃ³n ha terminado.
        
        Returns:
            True si la conversaciÃ³n estÃ¡ completa
        """
        return self.state.conversation_complete
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """
        Obtiene un resumen de la conversaciÃ³n.
        
        Returns:
            Diccionario con el resumen de la conversaciÃ³n
        """
        return {
            "responses": self.state.user_responses,
            "questions_asked": len(self.state.user_responses),
            "total_questions": len(self.state.pending_questions),
            "complete": self.state.conversation_complete,
            "messages_count": len(self.state.messages)
        }
    
    def reset_conversation(self):
        """Reinicia la conversaciÃ³n"""
        self.state = ConversationState()
        self.initialized = False


# FunciÃ³n de conveniencia para crear una instancia del agente
def create_questionnaire_agent(content: Dict[str, Any] = None) -> QuestionnaireAgent:
    """
    Crea una nueva instancia del agente de Questionnaire.
    
    Args:
        content: Contenido completo de la sesiÃ³n (welcome_message, questions, etc.)
    
    Returns:
        Instancia del agente configurada
    """
    logger.info(f"ğŸ¤– Creando agente con content completo (questions_data: {len(content.get('questions', []) if content else [])} preguntas)")
    return QuestionnaireAgent(content) 