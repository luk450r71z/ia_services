from typing import Dict, Any, List
from langchain_core.messages import HumanMessage, AIMessage
from langchain_groq import ChatGroq
import os
import datetime
import json

from ..models.conversation_models import ConversationState
from .tools.email_tool import simulate_email_send_direct
from ..utils.env_utils import load_env_variables

# Cargar variables de entorno al importar el m√≥dulo
load_env_variables()


class QuestionarieRHAgent:
    """
    Agente conversacional de RRHH.
    
    Este agente maneja entrevistas automatizadas de manera secuencial,
    recopila respuestas, decide cu√°ndo repreguntar y env√≠a res√∫menes por correo.
    """
    
    def __init__(self, questions: List[Any] = None):
        """
        Inicializa el agente de RRHH.
        
        Args:
            questions: Lista de preguntas (pueden ser strings o dicts con opciones)
        """
        self.state = ConversationState()
        self.initialized = False
        
        # Normalizar preguntas al formato interno
        if questions:
            self.questions_data = self._normalize_questions(questions)
            self.questions = [q["question"] for q in self.questions_data]
        else:
            self.questions_data = []
            self.questions = []
        
        # Guardar preguntas en metadatos
        if self.questions:
            self.state.extra_data["questions_data"] = self.questions_data
            print(f"üéØ Agente inicializado con {len(self.questions)} preguntas")
        else:
            print("üéØ Agente inicializado sin preguntas espec√≠ficas")
    
    def _normalize_questions(self, questions: List[Any]) -> List[Dict[str, Any]]:
        """Normaliza preguntas a formato interno consistente"""
        normalized = []
        for q in questions:
            if isinstance(q, str):
                # Pregunta simple como string
                normalized.append({"question": q, "options": None})
            elif isinstance(q, dict):
                # Ya est√° en formato dict
                normalized.append(q)
        return normalized
    
    @staticmethod
    def extract_questions(questions_data: Any) -> List[Dict[str, Any]]:
        """
        Extrae preguntas inteligentemente de cualquier formato JSON usando LLM.
        
        Args:
            questions_data: Datos en cualquier formato (array, object, nested)
            
        Returns:
            Lista de diccionarios con pregunta y opciones (si las hay)
            Formato: [{"question": "¬øPregunta?", "options": ["a", "b"] o None}]
            
        Raises:
            ValueError: Si no se puede extraer preguntas o no hay LLM disponible
        """
        # Convertir a JSON string
        json_str = json.dumps(questions_data, indent=2, ensure_ascii=False)
        
        # Configurar LLM (obligatorio)
        load_env_variables()
        groq_api_key = os.getenv("GROQ_API_KEY")
        
        if not groq_api_key:
            raise ValueError("‚ùå GROQ_API_KEY es requerida para extraer preguntas inteligentemente")
        
        print("üß† Extrayendo preguntas con opciones usando LLM")
        
        llm = ChatGroq(api_key=groq_api_key, model="llama-3.3-70b-versatile")
        
        prompt = f"""JSON_INPUT:
{json_str}

TASK: Extract questions and return JSON array only.

OUTPUT_FORMAT:
[{{"question": "text", "options": null}}, {{"question": "text", "options": ["a","b"]}}]

RETURN ONLY JSON ARRAY - NO OTHER TEXT"""
        
        response = llm.invoke(prompt)
        content = response.content.strip() if hasattr(response, 'content') else str(response).strip()
        
        try:
            # Extraer JSON de la respuesta (puede haber texto adicional)
            json_start = content.find('[')
            json_end = content.rfind(']') + 1
            
            if json_start == -1 or json_end == 0:
                raise ValueError("‚ùå No se encontr√≥ array JSON en la respuesta")
            
            json_content = content[json_start:json_end]
            questions_with_options = json.loads(json_content)
            
            if not questions_with_options:
                raise ValueError("‚ùå No se encontraron preguntas en el JSON proporcionado")
            
            print(f"‚úÖ Extra√≠das {len(questions_with_options)} preguntas con opciones")
            return questions_with_options
            
        except json.JSONDecodeError as e:
            raise ValueError(f"‚ùå Error parseando JSON: {json_content[:100]}...")
        except Exception as e:
            raise ValueError(f"‚ùå Error procesando respuesta del LLM: {str(e)}")
    
    def _format_question_with_options(self, question_index: int) -> str:
        """
        Formatea una pregunta con sus opciones (si las tiene)
        
        Args:
            question_index: √çndice de la pregunta
            
        Returns:
            Pregunta formateada con opciones
        """
        if not self.questions_data or question_index >= len(self.questions_data):
            return self.questions[question_index] if question_index < len(self.questions) else ""
        
        question_data = self.questions_data[question_index]
        question_text = question_data["question"]
        options = question_data.get("options")
        
        if options and isinstance(options, list):
            # Es pregunta de opci√≥n m√∫ltiple
            options_text = "\n".join([f"  ‚Ä¢ {option}" for option in options])
            return f"{question_text}\n\nOpciones:\n{options_text}\n\nPor favor, elige una de las opciones anteriores."
        else:
            # Es pregunta abierta
            return question_text
    
    def start_conversation(self) -> str:
        """
        Inicia una nueva conversaci√≥n.
        
        Returns:
            Mensaje inicial del agente
        """
        # Si ya est√° inicializado, devolver el mensaje de bienvenida sin volver a inicializar
        if self.initialized:
            print("‚úÖ Conversaci√≥n ya inicializada, devolviendo mensaje existente")
            welcome_content = """¬°Hola! Soy el asistente de RRHH de Adaptiera. 
Voy a realizarte algunas preguntas para conocerte mejor.
Responde con la mayor sinceridad posible.

Empecemos:"""
            if self.state.current_question:
                formatted_question = self._format_question_with_options(self.state.current_question_index)
                return f"{welcome_content}\n\n{formatted_question}"
            return welcome_content
        
        print("üöÄ Inicializando conversaci√≥n...")
        
        # Usar preguntas proporcionadas o cargar preguntas por defecto
        if self.questions:
            questions = self.questions
        else:
            # No hay preguntas configuradas
            print("‚ùå No hay preguntas configuradas para esta sesi√≥n")
            return "Error: No se han configurado preguntas para esta entrevista. Por favor, contacta al administrador."
        


        self.state.pending_questions = questions
        self.state.current_question_index = 0
        
        if questions:
            self.state.current_question = questions[0]
            
            # Mensaje de bienvenida
            welcome_content = """¬°Hola! Soy el asistente de RRHH de Adaptiera. 
Voy a realizarte algunas preguntas para conocerte mejor.
Responde con la mayor sinceridad posible.

Empecemos:"""
            
            welcome_message = AIMessage(content=welcome_content)
            self.state.messages.append(welcome_message)
            
            # Primera pregunta con opciones (si las tiene)
            formatted_question = self._format_question_with_options(0)
            question_message = AIMessage(content=formatted_question)
            self.state.messages.append(question_message)
            
            self.initialized = True
            return f"{welcome_message.content}\n\n{formatted_question}"
        
        return "¬°Hola! Soy el asistente de RRHH. ¬øC√≥mo puedo ayudarte?"
    
    def process_user_input(self, user_input: str) -> str:
        """
        Procesa la entrada del usuario y retorna la respuesta del agente.
        
        Args:
            user_input: Mensaje del usuario
            
        Returns:
            Respuesta del agente
        """
        if not self.initialized:
            return self.start_conversation()
        
        # Verificar si la conversaci√≥n ya est√° completa
        if self.state.conversation_complete:
            print("‚ö†Ô∏è Mensaje rechazado: la conversaci√≥n ya est√° completa")
            return "La entrevista ya ha finalizado. ¬°Gracias por tu participaci√≥n!"
        
        # Agregar mensaje del usuario al estado
        user_message = HumanMessage(content=user_input)
        self.state.messages.append(user_message)
        
        print(f"ü§î Procesando respuesta del usuario: {user_input[:50]}...")
        
        # Evaluar la respuesta
        is_satisfactory, clarification_reason = self._evaluate_response(user_input)
        
        if is_satisfactory:
            # Guardar respuesta satisfactoria
            self.state.user_responses[self.state.current_question] = user_input
            
            # Crear nombre de archivo para respuestas
            if self.questions:
                # Usar timestamp para hacer el archivo √∫nico
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                response_file = f"data/user_responses_custom_{timestamp}.json"
            else:
                response_file = "data/user_responses.json"
            
            # Las respuestas se guardan autom√°ticamente en la base de datos
            self.state.needs_clarification = False
            self.state.clarification_reason = None
            print(f"‚úÖ Respuesta aceptada para: {self.state.current_question}")
            
            # Avanzar a la siguiente pregunta
            return self._next_question()
        else:
            # Solicitar aclaraci√≥n
            self.state.needs_clarification = True
            self.state.clarification_reason = clarification_reason
            print(f"‚ùì Necesita clarificaci√≥n: {clarification_reason}")
            
            clarification_message = AIMessage(content=f"""Me gustar√≠a que puedas ampliar tu respuesta anterior.
{clarification_reason}

Por favor, proporciona m√°s detalles sobre: {self.state.current_question}""")
            
            self.state.messages.append(clarification_message)
            return clarification_message.content
    
    def _evaluate_response(self, user_response: str) -> tuple[bool, str]:
        """
        Eval√∫a si la respuesta del usuario es satisfactoria.
        Valida especialmente preguntas de opci√≥n m√∫ltiple.
        """
        current_question = self.state.current_question
        
        # Verificar si es pregunta de opci√≥n m√∫ltiple
        current_index = self.state.current_question_index
        if (self.questions_data and 
            current_index < len(self.questions_data)):
            
            question_data = self.questions_data[current_index]
            options = question_data.get("options")
            
            if options and isinstance(options, list):
                # Es pregunta de opci√≥n m√∫ltiple - validar respuesta
                return self._validate_multiple_choice(user_response, options)
        
        # Para preguntas abiertas, usar evaluaci√≥n con LLM
        return self._evaluate_open_question(user_response, current_question)
    
    def _validate_multiple_choice(self, user_response: str, options: List[str]) -> tuple[bool, str]:
        """Valida respuesta de opci√≥n m√∫ltiple usando LLM"""
        # Configurar LLM
        load_env_variables()
        groq_api_key = os.getenv("GROQ_API_KEY")
        
        if not groq_api_key:
            raise ValueError("‚ùå GROQ_API_KEY es requerida para validar opciones m√∫ltiples")
        
        llm = ChatGroq(api_key=groq_api_key, model="llama-3.3-70b-versatile")
        
        options_text = ", ".join(options)
        
        prompt = f"""
        Determina si la respuesta del usuario corresponde a alguna de las opciones v√°lidas.
        
        Opciones v√°lidas: {options_text}
        Respuesta del usuario: {user_response}
        
        Responde SOLO con:
        - "VALIDA" si la respuesta coincide con alguna opci√≥n (acepta variaciones, sin√≥nimos, etc.)
        - "INVALIDA" si no corresponde a ninguna opci√≥n
        
        S√© FLEXIBLE - acepta respuestas que claramente se refieren a una opci√≥n aunque no sean exactas.
        """
        
        response = llm.invoke(prompt)
        content = response.content.strip() if hasattr(response, 'content') else str(response).strip()
        
        if content.startswith("VALIDA"):
            print(f"‚úÖ Opci√≥n m√∫ltiple v√°lida: {user_response}")
            return True, ""
        else:
            print(f"‚ùå Opci√≥n m√∫ltiple inv√°lida: {user_response}")
            options_list = "\n".join([f"  ‚Ä¢ {opt}" for opt in options])
            return False, f"Debes elegir una de las siguientes opciones:\n{options_list}"
    
    def _evaluate_open_question(self, user_response: str, current_question: str) -> tuple[bool, str]:
        """Eval√∫a pregunta abierta con LLM"""
        load_env_variables()
        groq_api_key = os.getenv("GROQ_API_KEY")
        
        if not groq_api_key:
            # Fallback simple para preguntas abiertas
            is_satisfactory = len(user_response.strip()) > 3
            clarification_reason = "Por favor, proporciona una respuesta m√°s detallada." if not is_satisfactory else ""
            return is_satisfactory, clarification_reason
        
        llm = ChatGroq(api_key=groq_api_key, model="llama-3.3-70b-versatile")
        
        prompt = f"""
        Eval√∫a si la siguiente respuesta es satisfactoria para la pregunta planteada:
        
        Pregunta: {current_question}
        Respuesta: {user_response}
        
        Responde SOLO con:
        - "SATISFACTORIA" si la respuesta proporciona informaci√≥n b√°sica relevante
        - "NECESITA_CLARIFICACION: [raz√≥n espec√≠fica]" si est√° vac√≠a, es irrelevante o muy confusa
        
        S√© PERMISIVO en tu evaluaci√≥n. Acepta respuestas que tengan al menos alguna relaci√≥n con la pregunta, 
            incluso si son breves o no muy detalladas. Solo solicita clarificaci√≥n si la respuesta realmente 
            no tiene sentido o est√° completamente fuera de tema.
        """
        
        try:
            response = llm.invoke(prompt)
            content = response.content.strip() if hasattr(response, 'content') else str(response).strip()
            
            if content.startswith("SATISFACTORIA"):
                return True, ""
            elif content.startswith("NECESITA_CLARIFICACION"):
                reason = content.replace("NECESITA_CLARIFICACION:", "").strip()
                return False, reason
            else:
                # Formato inesperado, ser permisivo
                return True, ""
        except Exception as e:
            print(f"Error evaluando pregunta abierta: {e}")
            # Fallback simple
            is_satisfactory = len(user_response.strip()) > 3
            clarification_reason = "Por favor, proporciona una respuesta m√°s detallada." if not is_satisfactory else ""
            return is_satisfactory, clarification_reason
    
    def _next_question(self) -> str:
        """
        Avanza a la siguiente pregunta o finaliza la conversaci√≥n.
        
        Returns:
            Mensaje con la siguiente pregunta o finalizaci√≥n
        """
        print("‚û°Ô∏è Avanzando a la siguiente pregunta...")
        
        self.state.current_question_index += 1
        
        if self.state.current_question_index < len(self.state.pending_questions):
            # Hay m√°s preguntas
            self.state.current_question = self.state.pending_questions[self.state.current_question_index]
            
            # Formatear siguiente pregunta con opciones (si las tiene)
            formatted_question = self._format_question_with_options(self.state.current_question_index)
            
            next_question_message = AIMessage(content=f"""Perfecto, gracias por tu respuesta.

Siguiente pregunta:
{formatted_question}""")
            
            self.state.messages.append(next_question_message)
            return next_question_message.content
        else:
            # No hay m√°s preguntas, finalizar conversaci√≥n
            return self._finalize_conversation()
    
    def _finalize_conversation(self) -> str:
        """
        Finaliza la conversaci√≥n guardando respuestas y enviando correo.
        
        Returns:
            Mensaje de finalizaci√≥n
        """
        print("üèÅ Finalizando conversaci√≥n...")
        
        self.state.conversation_complete = True
        self.state.current_question = None
        
        # Enviar correo
        email_success = simulate_email_send_direct(self.state.user_responses)
        
        if email_success:
            final_message = AIMessage(content="""¬°Muchas gracias por tu tiempo! 

‚úÖ Tus respuestas han sido guardadas correctamente
‚úÖ Se ha enviado un resumen por correo electr√≥nico

Nuestro equipo de RRHH revisar√° tu informaci√≥n y se pondr√° en contacto contigo pronto.

¬°Que tengas un excelente d√≠a!""")
        else:
            final_message = AIMessage(content="""Gracias por completar la entrevista. 
Hubo algunos problemas t√©cnicos al procesar tu informaci√≥n, 
pero nuestro equipo se pondr√° en contacto contigo pronto.""")
        
        self.state.messages.append(final_message)
        return final_message.content
    
    def is_conversation_complete(self) -> bool:
        """
        Verifica si la conversaci√≥n ha terminado.
        
        Returns:
            True si la conversaci√≥n est√° completa
        """
        return self.state.conversation_complete
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """
        Obtiene un resumen de la conversaci√≥n.
        
        Returns:
            Diccionario con el resumen de la conversaci√≥n
        """
        return {
            "responses": self.state.user_responses,
            "questions_asked": len(self.state.user_responses),
            "total_questions": len(self.state.pending_questions),
            "complete": self.state.conversation_complete,
            "messages_count": len(self.state.messages)
        }
    
    def reset_conversation(self):
        """Reinicia la conversaci√≥n"""
        self.state = ConversationState()
        self.initialized = False


# Funci√≥n de conveniencia para crear una instancia del agente
def create_questionarie_rh_agent(questions: List[str] = None) -> QuestionarieRHAgent:
    """
    Crea una nueva instancia del agente de RRHH simplificado.
    
    Args:
        questions: Lista de preguntas espec√≠ficas para la entrevista
    
    Returns:
        Instancia del agente configurada
    """
    return QuestionarieRHAgent(questions) 